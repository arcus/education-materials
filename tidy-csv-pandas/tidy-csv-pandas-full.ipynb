{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to tabular .csv data with pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome! In this lesson, you'll get a crash course in bringing your tabular research data into Python. \n",
    "\n",
    "We will introduce **pandas**, a key tool in the Python ecosystem, along with **NumPy** which is the powerful framework underlying pandas. Throughout this notebook, we will focus on how to leverage Python tools to think critically about your data. \n",
    "\n",
    "At the end, you'll have a dataset that is ready for exploratory data analysis and visualization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('https://d33wubrfki0l68.cloudfront.net/795c039ba2520455d833b4034befc8cf360a70ba/558a5/diagrams/data-science-explore.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The exploratory data analysis cycle (Grolemund and Wickham, 2017)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you take a look at the exploratory data cycle above, this session will focus on the **import**, **tidy**, **transform**, and **visualize** steps. (If you're coming from the R world, you may recognize this model from Hadley Wickham's unmissable [R for Data Science](https://r4ds.had.co.nz/)!\n",
    "\n",
    "But first, let's start with our research questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study: Breast Cancer Dianostic Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does it take to produce good, reliable diagnoses? Clinicians need to use information strategically to determine the best plan of care. Researchers may wish to improve clinician training, invent automated systems that support patient-facing staff, or identify pathways for future investigations into the etiology of disease.\n",
    "\n",
    "When exploring a new dataset, it's important to understand and clearly articulate research questions. \n",
    "\n",
    "Let's suppose we are researchers at Big University Oncology Institute working with fine needle aspirate (FNA) images of breast masses. Our long-term goal is to increase the efficacy of distinguishing malignant from non-malignant masses from FNA digital images. As one part of that research, we are interested in whether patterns exist in imaging data that would improve the ability of clinicians to correctly dinstinguish malignant and non-malignant tumors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('https://upload.wikimedia.org/wikipedia/commons/8/8b/Breast_fibroadenoma_by_fine_needle_aspiration_%282%29_PAP_stain.jpg', width = 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fine needle aspiration of fibroadenoma, a type of benign breast tumor. (Source: Wikimedia)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we'll be using the **Wisconsin Diagnostic Breast Cancer (WDBC) dataset**. This data was initially captured in eight groups from January 1989 to November 1991 by Dr. William H. Wolberg at the University of Wisconsin Hospitals, Madison. Dr. Wolberg subsequently donated the dataset to the University of California-Irvine's Machine Learning Repository, where it has been used in dozens of subsequent articles and further annalyzed and transformed.\n",
    "\n",
    "The [original repository](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+\\(Diagnostic\\)) contains several versions of the breast cancer data. For today we're interested in Dr. Wolberg's original encodings of the FNA image data, which uses a ranking of 1 to 10 to describe a variety of attributes such unifority of cell shape and size, mitoses and normal nuclei count, margin adhesion, clump thickness, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our research questions, how might we wish to explore a dataset where observations correspond to fine needle aspiration (FNA) images of breast growths? What's important to us?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(**Edit this box to include a question you would like to investigate in this data.**)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this dataset, we have access to two files: \n",
    "\n",
    "1. the dataset, represented as a comma-separated value (CSV) file, and \n",
    "2. a plain-text readme file describing the dataset. \n",
    "\n",
    "I'll include relevant excerpts of the readme file as we go, but you can read the full readme in the Appendix below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing your data into a pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started. First, let's import the pandas and numpy library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python likes to think in terms of objects. So when we bring the functionality of pandas into our Notebook, we will find ourselves constantly accessing functionality of the pandas library via code like pandas.DataFrame or pandas.readcsv(). To make our collective lives easier, there is a convention of assigning pandas the alias pd. (And then let's do the same thing for numpy)\n",
    "\n",
    "Python includes several ways to import CSV data. In this case, we want to bring our data into a special pandas data structure called a **DataFrame**. DataFrames are similar to tables, in that they store a collection of observations with regular sets of attributes. But in pandas, they have special functionality and are optimized for extremely quick access.\n",
    "\n",
    "To create a DataFrame from existing tabular data, we only one line of code with the following parts:\n",
    "\n",
    "1. A variable to assign the outcome of our pandas function (in this case, a DataFrame).\n",
    "2. A call to the appropriate CSV reading method of pandas, and\n",
    "3. Passing in the location (or filename) of our CSV file as a parameter.\n",
    "\n",
    "For this example, our csv data is located in this same directory as **wisconsin_data.csv**. Run the code block below to generate a DataFrame and assign it to the variable **diagnostic_data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wi_data_url = 'https://raw.githubusercontent.com/arcus/education-materials/master/tidy-csv-pandas/wisconsin_data.csv'\n",
    "diagnostic_data = pd.read_csv(wi_data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've created a new DataFrame object - remember, everything in Python is an object! - consisting of data from our diagnostic_data table.\n",
    "\n",
    "In pandas, a DataFrame is a data type that allows us to use special functions. When a function is relative to an object, we call it a **method** and access it with a period at the end of the object. So, for example, print is a standalone function:\n",
    "\n",
    "```\n",
    "print(\"Hello world\")\n",
    "```\n",
    "\n",
    "But .head() is a pandas **method** that must be called on a pandas DataFrame:\n",
    "\n",
    "```\n",
    "diagnostic_data.head()\n",
    "```\n",
    "\n",
    ".head() usefully shows us the first several rows in our DataFrame along with row and column labels. Let's try it below, passing in `20` to show the first 20 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no! Take a look at the header line. What do you think is going on here?\n",
    "\n",
    "pandas makes certain assumptions by default when loading in data. In this case, pandas assumed that the top row of our data contains a header. In fact, our data is missing a header! Let's fix this.\n",
    "\n",
    "First, let's redo our earlier read_csv() method call. We want to pass in two parameters:\n",
    "\n",
    "* Our filename is \"wisconsin_data.csv\". We need to put this parameter first, but it doesn't need a name.\n",
    "* We need to pass in the \"header\" parameter as **None**. (None doesn't need quotation marks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add both parameters to .read_csv()\n",
    "diagnostic_data = pd.read_csv(wi_data_url, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, halfway there! We can use the DataFrame as is, but it will be frustrating not to rely on actual column names. We need to dig into the data documentation itself to figure out the best labels in this case. \n",
    "\n",
    "Here's the relevant part of our data readme:\n",
    "\n",
    "```\n",
    "7. Attribute Information: (class attribute has been moved to last column)\n",
    "\n",
    "   #  Attribute                     Domain\n",
    "   -- -----------------------------------------\n",
    "   1. Sample code number            id number\n",
    "   2. Clump Thickness               1 - 10\n",
    "   3. Uniformity of Cell Size       1 - 10\n",
    "   4. Uniformity of Cell Shape      1 - 10\n",
    "   5. Marginal Adhesion             1 - 10\n",
    "   6. Single Epithelial Cell Size   1 - 10\n",
    "   7. Bare Nuclei                   1 - 10\n",
    "   8. Bland Chromatin               1 - 10\n",
    "   9. Normal Nucleoli               1 - 10\n",
    "  10. Mitoses                       1 - 10\n",
    "  11. Class:                        (2 for benign, 4 for malignant)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assign a header to a DataFrame without column labels (or to replace existing Column labels), we can use a list of strings.\n",
    "\n",
    "Note that it's a good practice to write our column labels without any spaces. This lets us access columns more flexibly later on (for instance, to extract just \"column A\" `data_frame.column A` won't work, but `data_frame['column A']` will work. In contrast, both `data_frame.col_a` and `data_frame['col_a']` will work as expected.\n",
    "\n",
    "ALSO, note that in many programming languages, \"class\" is a **protected** term. That means it can cause unexpected behavior as a string because Python may assume you're referencing the programmatic term \"class\". It's a good practice to rename 'class' to something else, in case we want to use the notation style `data_frame.class` in the future.\n",
    "\n",
    "Run the code below to generate a list of strings and assign them as labels to the diagnostic_data DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_labels = ['sample_code', \n",
    "                    'clump_thickness', \n",
    "                    'uniformity_cell_shape', \n",
    "                    'uniformity_cell_size',\n",
    "                    'marginal_adhesion',\n",
    "                    'single_epi_cell_size',\n",
    "                    'bare_nuclei',\n",
    "                    'bland_chrom',\n",
    "                    'norm_nuclei',\n",
    "                    'mitoses',\n",
    "                    'malignant'\n",
    "                   ]\n",
    "\n",
    "diagnostic_data.columns = attribute_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a nicely labeled table output to the Notebook above. Congrats! Your data is definitively imported into a DataFrame.\n",
    "\n",
    "Note that pandas will create an index and assign ordered values beginning at 0 by default. You can override this behavior (as in, you could set sample_code as the index) but it's helpful to have this index in place for future use, e.g. if we wanted to reverse the order of our data and sample_code wasn't sequential. [Here is a helpful resource](https://t.co/swUeOFoc33?amp=1) for learning more about index behavior in pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidying data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have likely encountered data that is unwieldly, unreliable, and not ready for analysis. (Some of this might be your own research data!) \n",
    "\n",
    "If we think of this difficult data as **messy data**, we might assume that tidy data is the opposite: usable, reliable, and ready for analysis. This is certainly the desired outcome of tidying data. However, **tidy data** also refers to a specific paradigm of structuring tabular data, put forward by Hadley Wickham in 2013. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For us to consider a dataset a tidy dataset, it must follow three principles (Wickham, 2013):\n",
    "\n",
    "> 1. Each variable forms a column.\n",
    "> 2. Each observation forms a row.\n",
    "> 3. Each type of observational unit forms a table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also express this visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('https://d33wubrfki0l68.cloudfront.net/6f1ddb544fc5c69a2478e444ab8112fb0eea23f8/91adc/images/tidy-1.png', width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Source: [\"12. Tidy Data\"](https://r4ds.had.co.nz/tidy-data.html), *R for Data Science*. Wickham and Grolemund, 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already made one significant change to make our DataFrame **tidy** - manually declaring a header! If we leave the distinction between header and first row ambiguous, we lose the integrity of the tidy data structure where each row corresponds to an observation, and *only* an observation.\n",
    "\n",
    "Another concern is empty, missing, or **null values**. \n",
    "\n",
    "If the null value \"belongs\" to an observation, in that the value was in fact the varaible recorded for that observation, then this is completely acceptable and we must simply use a reliable way to encode this null value. If the null value is, instead, the product of missing, damaged, or incomplete data, then we may wish to treat that null differently (such as use it for evidence for excluding this observation, if experimentally appropriate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hunting for nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, it is important to remember that pandas is built on top of NumPy, a powerful numeric computing package that relies upon the highly-optimized NumPy array data structure for things like vector math. NumPy only has native support for null values in floating-point numbers. For all other data types, including integers and strings, pandas has filled in the gaps, and it can be clunky at times.\n",
    "\n",
    "The key takeaway: do not assume nulls behave the way you expect them to in pandas, and always check data types!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following example. To check for null values in our diagnostic data, we may wish to use the following syntax: `diagnostic_data.isnull().any()`. What this does:\n",
    "\n",
    "* 'diagnostic_data' is our DataFrame of FNA image data observations\n",
    "* `.isnull()` is a method that returns True or False if a null value is found, per value in our dataframe\n",
    "* `.any()` looks for a single True value in any column and returns the outcome.\n",
    "\n",
    "Let's look at the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_data.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, right? I noticed that we had forgotten to check our data types beforehand. Let's do that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.info()` method reiterates what we saw before: a complete set of non-null values per column **CHECKCHECK** but we also learn the datatype that pandas assigned each column at the `read_csv()` step. Most columns are **int64**, which is a 64-bit integer and wholly appropriate for our data. However, bare_nuclei is an object! \n",
    "\n",
    "By default, pandas converts alphanumeric or what Python considers \"string\" data into object format on importing. This means there is likely some non-numeric data in our bare_nuclei column. Let's look at all unique values in this column to investigate.\n",
    "\n",
    "The `.unique()` method can be used on a DataFrame column to show all unique values. For instance, if we want to see all of the unique values for mitoses, we could run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_data.mitoses.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at bare_nuclei. Write and run the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code below that calls the unique method on the column we are investigating:\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahh! It appears that some values contain a question mark instead of an integer. To chck how prevelant this ? is in our data, let's replace the `.unique()` method call with `.value_counts()`, which provides counts for each unique value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code below that generates counts for each unique value in the column we are investigating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this occurs in 16 of our observations. If we reference the data documentation, we'll see that this was foregrounded for us ahead of time (but the documentation did not specify in which column the missing attribute values occur!)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "8. Missing attribute values: 16\n",
    "\n",
    "   There are 16 instances in Groups 1 to 6 that contain a single missing \n",
    "   (i.e., unavailable) attribute value, now denoted by \"?\".  \n",
    "\n",
    "```\n",
    "One important takeaway: **always read the data documentation!!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we deal with these nulls, and also go back to treating `bare_nuclei` values as numeric? Our best option in pandas is generally to use the NumPy NaN or null-value approach. \n",
    "\n",
    "This is subject to an important limitation: if we wish to use nulls alongside numeric data, np.NaN values **must** be floating-point. Integer won't cut it in this case. We could, alternatively, decide our data is of 'object' type which can handle a wide variety of possibilities, but then we would lose some functionality later on w/r/t histograms and correlation plots.\n",
    "\n",
    "Luckily, we only need to use two methods to create our desired numeric data with nulls:\n",
    "\n",
    "* `.replace()` to convert all question marks with a NumPy NaN object\n",
    "* `.astype()` which will allow us to coerce eligible data to floating point numeric, or 'float64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_data.bare_nuclei = diagnostic_data.bare_nuclei.replace('?', np.nan)\n",
    "diagnostic_data.bare_nuclei = diagnostic_data.bare_nuclei.astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check to see if our DataFrame now includes nulls that pandas can recognize, and also stores bare-nuclei in the correct data type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_data.bare_nuclei.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The core data exploration cycle: transforming and visualizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier, we had described our research question as follows:\n",
    "\n",
    "> ...we are interested in whether patterns exist in imaging data that would improve the ability of clinicians to correctly dinstinguish malignant and non-malignant tumors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break this goal into some specific tasks. To learn more, we may wish to learn:\n",
    "1. The overall size, shape, and scope of our FNA imaging data.\n",
    "2. Summary statistics and distributions for each image attribute.\n",
    "3. Possible correlations between image attributes.\n",
    "\n",
    "And, in the future, to pursue:\n",
    "4. Avenues for subsequent analysis (e.g. designing a machine learning classification task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've brought our data into the DataFrame structure, we have access to an array of methods and tools to accomplish these tasks. Let's explore each below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Overall size, shape, and scope of our FNA imaging data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When getting oriented to a new dataset, I like to see a few examples of observations, and also make sure the data is in a format I expect.\n",
    "\n",
    "We've already seen `.head()` and `.info()`. Let's also add in `.tail()`, which just shows the bottom 5 items. We can also pass in a parameter to see the nth rows from the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to quickly view the overall dimensions of your DataFrame, the most concise way is to use .shape . Unlike .head(), which is a **method** (or function that belongs to our DataFrame object), .shape is an **atribute** meaning that it returns some data about the object. \n",
    "\n",
    "The reason this matters is that .head() always takes parentheses at the end, and .shape never takes paranetheses.\n",
    "\n",
    "Run the code below to see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast, writing `.shape` with paranetheses will produce a TypeError:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_data.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# replace with your code\n",
    "diagnostic_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data in a tidy dataframe, those observations include a outcome class variable as well as several storng candidate for predictor variables. This is an excellent task to pursue via the supervised machine learning method of [binary classification](https://www.sciencedirect.com/topics/computer-science/binary-classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Python ecosystem, most if not all popular machine learning packages will work natively with pandas DataFrames. Look out for a future lesson using random forest classifers with scikit-learn on this very dataset you have prepared!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_data.to_csv(\"wisconsin_data_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Works cited & further reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Full readme file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full readme file accompanying data:\n",
    "\n",
    "```\n",
    "Citation Request:\n",
    "   This breast cancer databases was obtained from the University of Wisconsin\n",
    "   Hospitals, Madison from Dr. William H. Wolberg.  If you publish results\n",
    "   when using this database, then please include this information in your\n",
    "   acknowledgements.  Also, please cite one or more of:\n",
    "\n",
    "   1. O. L. Mangasarian and W. H. Wolberg: \"Cancer diagnosis via linear \n",
    "      programming\", SIAM News, Volume 23, Number 5, September 1990, pp 1 & 18.\n",
    "\n",
    "   2. William H. Wolberg and O.L. Mangasarian: \"Multisurface method of \n",
    "      pattern separation for medical diagnosis applied to breast cytology\", \n",
    "      Proceedings of the National Academy of Sciences, U.S.A., Volume 87, \n",
    "      December 1990, pp 9193-9196.\n",
    "\n",
    "   3. O. L. Mangasarian, R. Setiono, and W.H. Wolberg: \"Pattern recognition \n",
    "      via linear programming: Theory and application to medical diagnosis\", \n",
    "      in: \"Large-scale numerical optimization\", Thomas F. Coleman and Yuying\n",
    "      Li, editors, SIAM Publications, Philadelphia 1990, pp 22-30.\n",
    "\n",
    "   4. K. P. Bennett & O. L. Mangasarian: \"Robust linear programming \n",
    "      discrimination of two linearly inseparable sets\", Optimization Methods\n",
    "      and Software 1, 1992, 23-34 (Gordon & Breach Science Publishers).\n",
    "\n",
    "1. Title: Wisconsin Breast Cancer Database (January 8, 1991)\n",
    "\n",
    "2. Sources:\n",
    "   -- Dr. WIlliam H. Wolberg (physician)\n",
    "      University of Wisconsin Hospitals\n",
    "      Madison, Wisconsin\n",
    "      USA\n",
    "   -- Donor: Olvi Mangasarian (mangasarian@cs.wisc.edu)\n",
    "      Received by David W. Aha (aha@cs.jhu.edu)\n",
    "   -- Date: 15 July 1992\n",
    "\n",
    "3. Past Usage:\n",
    "\n",
    "   Attributes 2 through 10 have been used to represent instances.\n",
    "   Each instance has one of 2 possible classes: benign or malignant.\n",
    "\n",
    "   1. Wolberg,~W.~H., \\& Mangasarian,~O.~L. (1990). Multisurface method of \n",
    "      pattern separation for medical diagnosis applied to breast cytology. In\n",
    "      {\\it Proceedings of the National Academy of Sciences}, {\\it 87},\n",
    "      9193--9196.\n",
    "      -- Size of data set: only 369 instances (at that point in time)\n",
    "      -- Collected classification results: 1 trial only\n",
    "      -- Two pairs of parallel hyperplanes were found to be consistent with\n",
    "         50% of the data\n",
    "         -- Accuracy on remaining 50% of dataset: 93.5%\n",
    "      -- Three pairs of parallel hyperplanes were found to be consistent with\n",
    "         67% of data\n",
    "         -- Accuracy on remaining 33% of dataset: 95.9%\n",
    "\n",
    "   2. Zhang,~J. (1992). Selecting typical instances in instance-based\n",
    "      learning.  In {\\it Proceedings of the Ninth International Machine\n",
    "      Learning Conference} (pp. 470--479).  Aberdeen, Scotland: Morgan\n",
    "      Kaufmann.\n",
    "      -- Size of data set: only 369 instances (at that point in time)\n",
    "      -- Applied 4 instance-based learning algorithms \n",
    "      -- Collected classification results averaged over 10 trials\n",
    "      -- Best accuracy result: \n",
    "         -- 1-nearest neighbor: 93.7%\n",
    "         -- trained on 200 instances, tested on the other 169\n",
    "      -- Also of interest:\n",
    "         -- Using only typical instances: 92.2% (storing only 23.1 instances)\n",
    "         -- trained on 200 instances, tested on the other 169\n",
    "\n",
    "4. Relevant Information:\n",
    "\n",
    "   Samples arrive periodically as Dr. Wolberg reports his clinical cases.\n",
    "   The database therefore reflects this chronological grouping of the data.\n",
    "   This grouping information appears immediately below, having been removed\n",
    "   from the data itself:\n",
    "\n",
    "     Group 1: 367 instances (January 1989)\n",
    "     Group 2:  70 instances (October 1989)\n",
    "     Group 3:  31 instances (February 1990)\n",
    "     Group 4:  17 instances (April 1990)\n",
    "     Group 5:  48 instances (August 1990)\n",
    "     Group 6:  49 instances (Updated January 1991)\n",
    "     Group 7:  31 instances (June 1991)\n",
    "     Group 8:  86 instances (November 1991)\n",
    "     -----------------------------------------\n",
    "     Total:   699 points (as of the donated datbase on 15 July 1992)\n",
    "\n",
    "   Note that the results summarized above in Past Usage refer to a dataset\n",
    "   of size 369, while Group 1 has only 367 instances.  This is because it\n",
    "   originally contained 369 instances; 2 were removed.  The following\n",
    "   statements summarizes changes to the original Group 1's set of data:\n",
    "\n",
    "   #####  Group 1 : 367 points: 200B 167M (January 1989)\n",
    "   #####  Revised Jan 10, 1991: Replaced zero bare nuclei in 1080185 & 1187805\n",
    "   #####  Revised Nov 22,1991: Removed 765878,4,5,9,7,10,10,10,3,8,1 no record\n",
    "   #####                  : Removed 484201,2,7,8,8,4,3,10,3,4,1 zero epithelial\n",
    "   #####                  : Changed 0 to 1 in field 6 of sample 1219406\n",
    "   #####                  : Changed 0 to 1 in field 8 of following sample:\n",
    "   #####                  : 1182404,2,3,1,1,1,2,0,1,1,1\n",
    "\n",
    "5. Number of Instances: 699 (as of 15 July 1992)\n",
    "\n",
    "6. Number of Attributes: 10 plus the class attribute\n",
    "\n",
    "7. Attribute Information: (class attribute has been moved to last column)\n",
    "\n",
    "   #  Attribute                     Domain\n",
    "   -- -----------------------------------------\n",
    "   1. Sample code number            id number\n",
    "   2. Clump Thickness               1 - 10\n",
    "   3. Uniformity of Cell Size       1 - 10\n",
    "   4. Uniformity of Cell Shape      1 - 10\n",
    "   5. Marginal Adhesion             1 - 10\n",
    "   6. Single Epithelial Cell Size   1 - 10\n",
    "   7. Bare Nuclei                   1 - 10\n",
    "   8. Bland Chromatin               1 - 10\n",
    "   9. Normal Nucleoli               1 - 10\n",
    "  10. Mitoses                       1 - 10\n",
    "  11. Class:                        (2 for benign, 4 for malignant)\n",
    "\n",
    "8. Missing attribute values: 16\n",
    "\n",
    "   There are 16 instances in Groups 1 to 6 that contain a single missing \n",
    "   (i.e., unavailable) attribute value, now denoted by \"?\".  \n",
    "\n",
    "9. Class distribution:\n",
    " \n",
    "   Benign: 458 (65.5%)\n",
    "   Malignant: 241 (34.5%)\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
