{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore and visualize with pandas + seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome! In this lesson, you'll get a crash course in bringing your tabular research data into Python. \n",
    "\n",
    "We will introduce **pandas**, a key tool in the Python ecosystem, along with **NumPy** which is the powerful framework underlying pandas and **matplotlib**, a visualization library already integrated into pandas. (We'll use the **seaborn** visualization library briefly too). Throughout this notebook, we will focus on how to leverage Python tools to think critically about your data. \n",
    "\n",
    "At the end, you'll have written a data science recipe you can bring to future datasets and research questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('https://d33wubrfki0l68.cloudfront.net/795c039ba2520455d833b4034befc8cf360a70ba/558a5/diagrams/data-science-explore.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The exploratory data analysis cycle (Grolemund and Wickham, 2017)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A refresher: Breast Cancer Dianostic Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(If you're proceeding directly from the Import and Tidy Data pandas notebook, feel free to skip ahead to **Load in cleaned data from .csv**)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('https://upload.wikimedia.org/wikipedia/commons/8/8b/Breast_fibroadenoma_by_fine_needle_aspiration_%282%29_PAP_stain.jpg', width = 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fine needle aspiration of fibroadenoma, a type of benign breast tumor. (Source: Wikimedia)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we'll be using the **Wisconsin Diagnostic Breast Cancer (WDBC) dataset**. This data was initially captured in eight groups from January 1989 to November 1991 by Dr. William H. Wolberg at the University of Wisconsin Hospitals, Madison. Dr. Wolberg subsequently donated the dataset to the University of California-Irvine's Machine Learning Repository, where it has been used in dozens of subsequent articles and further annalyzed and transformed.\n",
    "\n",
    "The [original repository](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+\\(Diagnostic\\)) contains several versions of the breast cancer data. For today we're interested in Dr. Wolberg's original encodings of the FNA image data, which uses a ranking of 1 to 10 to describe a variety of attributes such unifority of cell shape and size, mitoses and normal nuclei count, margin adhesion, clump thickness, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in cleaned data from .csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, we took the raw data from the Wisconsin Diagnostic Breast Cancer (WDBC) dataset and cleaned & tidied it. This lesson will continue with the cleaned data. To start, let's load the results of the previous notebook into a pandas DataFrame:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started. First, let's import the pandas and numpy library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_data = pd.read_csv('https://raw.githubusercontent.com/arcus/education-materials/master/explore-pandas-seaborn/wisconsin_data_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our data. How does it look? Use the `.head()` method to investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# look at head of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm. It looks like we have an extra column at the front of our data. My guess is that we didn't omit indexes when exporting to .csv before). Let's drop this unnamed first column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_data = diagnostic_data.drop(diagnostic_data.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The core data exploration cycle: transforming and visualizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break this goal into some specific tasks. To learn more, we may wish to learn:\n",
    "1. The overall size, shape, and scope of our FNA imaging data.\n",
    "2. Summary statistics and distributions for each image attribute.\n",
    "3. Possible correlations between image attributes.\n",
    "\n",
    "And, in the future:\n",
    "4. Avenues for subsequent analysis (e.g. designing a machine learning classification task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Overall size, shape, and scope of our FNA imaging data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When getting oriented to a new dataset, I like to see a few examples of observations, and also make sure the data is in a format I expect.\n",
    "\n",
    "We've already seen `.head()` and `.info()`. Let's also add in `.tail()`, which just shows the bottom 5 items. We can also pass in a parameter to see the nth rows from the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to quickly view the overall dimensions of your DataFrame, the most concise way is to use .shape . Unlike .head(), which is a **method** (or function that belongs to our DataFrame object), .shape is an **atribute** meaning that it returns some data about the object. \n",
    "\n",
    "The reason this matters is that .head() always takes parentheses at the end, and .shape never takes paranetheses.\n",
    "\n",
    "Run the code below to see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast, writing `.shape` with paranetheses will produce a TypeError:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_data.shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Summary statistics and distributions for each image attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a big-picture view of how our dataframe is organized, but we don't yet know much about the individual columns. What is the distribution of values within a given column? How do they compare to one another?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, column-level analysis is where the NumPy-powered architecture of pandas really shines. Let's take some time exploring columns individually and comparing them to one another.\n",
    "\n",
    "First, run the `.columns` attribute call on our dataframe to remind us of all available columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with your code\n",
    "diagnostic_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get a visual sense of the distirbution of values within each column. Pandas supports visualization using the matplotlib library natively, which is very helpful for our purposes. To make this work within a notebook, we simply need to import matplotlib, and then specify that we want to see inline visualizations with a \"magic command\" (the magic command is specific to Jupyter Notebooks):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate side-by-side histograms of all our columns by calling the `.hist()` method on our DataFrame. It's not necessary to specify paramters, but for readability, it's helpful to set the `figsize` parameter to an `(x, y)` tuple, in inches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_plot = diagnostic_data.hist(figsize=(12,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pair this code with the column select syntax `DataFrame[[\"Column A\", \"Column B\"...]]` to compare just a subset of columns. Unlike the dot format of referencing a column (which is my favorite for most situations), the bracket format allows you to pass in a list of columns, which is helpful for subsetting. If you write code this way, note that you need two brackets, and the column names must now be passed in as strings (and thus must be surrounded by quotation marks). \n",
    "\n",
    "Run the example below and please experiment with your own code afterwards:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_plot = diagnostic_data[[\"marginal_adhesion\",\"uniformity_cell_size\"]].hist(figsize=(12,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add your own code to compare the distribution of two other variables here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the `.describe()` method specifically on a column to get summary statistics. For instance, here is the summary statistics on the `clump_thickness` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_data['clump_thickness'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also write this method call using the bracket notation for referencing a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your own code to view a column's summary statistics, or to compare multiple columns of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Possible correlations between image attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize correlations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(diagnostic_data.corr())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = diagnostic_data.corr()\n",
    "sns.heatmap(corr, \n",
    "        xticklabels=corr.columns,\n",
    "        yticklabels=corr.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code for a new diagnostic_predictors DataFrame\n",
    "# grab column labels we want\n",
    "# use this to grab data we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# generate a new correlation heatmap based on the new dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing distributions of variables (features) by category (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok, let's zero in on plotting our variable distributions. sns.kdeplot()\n",
    "\n",
    "sns.kdeplot(diagnostic_data.mitoses, shade=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's create kdeplots for each category and superimpose them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's loop over our predictor variables and create these plots one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = diagnostic_data.columns[1:-2]\n",
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looping pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about using histograms, making countplots, etc? We have other tools but need to be mindful of scaling issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"norm_nuclei\", hue=\"is_malignant\", data=diagnostic_data, palette=\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in predictors:\n",
    "    plt.figure()\n",
    "    \n",
    "    sns.countplot(x=feature, hue=\"is_malignant\", data=diagnostic_data, palette=\"colorblind\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**But wait! What should we do about the scale differences here??**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[GitHub issue: \"standardizing\" a countplot](https://github.com/mwaskom/seaborn/issues/1027#issuecomment-360866896)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x is variable of interest\n",
    "# y can be whatever want it to be, such as the word 'prop'\n",
    "# hue is the categorical var\n",
    "\n",
    "#(diagnostic_data[x]\n",
    "# .groupby(diagnostic_data[hue])\n",
    "# .value_counts(normalize=True)\n",
    "# .rename(y)\n",
    "# .reset_index()\n",
    "# .pipe((sns.barplot, \"data\"), x=x, y=y, hue=hue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to go from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've tidied up our dataset, including dealing with headers and nulls; glimpsed the design, shape, and typical observations within our datset; explored summary statistics and distributions of values within columns; and evaluated covariance between columns with correlation plots and heat maps. \n",
    "\n",
    "Each of these tasks are valuable in their own right, no matter what analytical method we wish to pursue next. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Avenues for subsequent analysis (e.g. designing a machine learning classification task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data work you've done in pandas is *particularly* relevant if we plan to approach this research question\n",
    "\n",
    "> we are interested in whether patterns exist in imaging data that would improve the ability of clinicians to correctly dinstinguish malignant and non-malignant tumors\n",
    "\n",
    "as the classification task\n",
    "\n",
    "> Given the output class of \"malignant\" or \"benign\" in our observations, can we generate an accurate and reliable prediction from observations about Fine Needle Aspiration images (also contained within our observations)? If so, what variables are singificant in predicting an outcome class effectively? Among those important predictors, how would a change in measurement value influence the likely outcome, and how confident are we in that influence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data in a tidy dataframe, those observations include a outcome class variable as well as several storng candidate for predictor variables. This is an excellent task to pursue via the supervised machine learning method of [binary classification](https://www.sciencedirect.com/topics/computer-science/binary-classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Python ecosystem, most if not all popular machine learning packages will work natively with pandas DataFrames. Look out for a future lesson using random forest classifers with scikit-learn on this very dataset you have prepared!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_data.to_csv(\"wisconsin_data_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Works cited & further reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Full readme file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full readme file accompanying data:\n",
    "\n",
    "```\n",
    "Citation Request:\n",
    "   This breast cancer databases was obtained from the University of Wisconsin\n",
    "   Hospitals, Madison from Dr. William H. Wolberg.  If you publish results\n",
    "   when using this database, then please include this information in your\n",
    "   acknowledgements.  Also, please cite one or more of:\n",
    "\n",
    "   1. O. L. Mangasarian and W. H. Wolberg: \"Cancer diagnosis via linear \n",
    "      programming\", SIAM News, Volume 23, Number 5, September 1990, pp 1 & 18.\n",
    "\n",
    "   2. William H. Wolberg and O.L. Mangasarian: \"Multisurface method of \n",
    "      pattern separation for medical diagnosis applied to breast cytology\", \n",
    "      Proceedings of the National Academy of Sciences, U.S.A., Volume 87, \n",
    "      December 1990, pp 9193-9196.\n",
    "\n",
    "   3. O. L. Mangasarian, R. Setiono, and W.H. Wolberg: \"Pattern recognition \n",
    "      via linear programming: Theory and application to medical diagnosis\", \n",
    "      in: \"Large-scale numerical optimization\", Thomas F. Coleman and Yuying\n",
    "      Li, editors, SIAM Publications, Philadelphia 1990, pp 22-30.\n",
    "\n",
    "   4. K. P. Bennett & O. L. Mangasarian: \"Robust linear programming \n",
    "      discrimination of two linearly inseparable sets\", Optimization Methods\n",
    "      and Software 1, 1992, 23-34 (Gordon & Breach Science Publishers).\n",
    "\n",
    "1. Title: Wisconsin Breast Cancer Database (January 8, 1991)\n",
    "\n",
    "2. Sources:\n",
    "   -- Dr. WIlliam H. Wolberg (physician)\n",
    "      University of Wisconsin Hospitals\n",
    "      Madison, Wisconsin\n",
    "      USA\n",
    "   -- Donor: Olvi Mangasarian (mangasarian@cs.wisc.edu)\n",
    "      Received by David W. Aha (aha@cs.jhu.edu)\n",
    "   -- Date: 15 July 1992\n",
    "\n",
    "3. Past Usage:\n",
    "\n",
    "   Attributes 2 through 10 have been used to represent instances.\n",
    "   Each instance has one of 2 possible classes: benign or malignant.\n",
    "\n",
    "   1. Wolberg,~W.~H., \\& Mangasarian,~O.~L. (1990). Multisurface method of \n",
    "      pattern separation for medical diagnosis applied to breast cytology. In\n",
    "      {\\it Proceedings of the National Academy of Sciences}, {\\it 87},\n",
    "      9193--9196.\n",
    "      -- Size of data set: only 369 instances (at that point in time)\n",
    "      -- Collected classification results: 1 trial only\n",
    "      -- Two pairs of parallel hyperplanes were found to be consistent with\n",
    "         50% of the data\n",
    "         -- Accuracy on remaining 50% of dataset: 93.5%\n",
    "      -- Three pairs of parallel hyperplanes were found to be consistent with\n",
    "         67% of data\n",
    "         -- Accuracy on remaining 33% of dataset: 95.9%\n",
    "\n",
    "   2. Zhang,~J. (1992). Selecting typical instances in instance-based\n",
    "      learning.  In {\\it Proceedings of the Ninth International Machine\n",
    "      Learning Conference} (pp. 470--479).  Aberdeen, Scotland: Morgan\n",
    "      Kaufmann.\n",
    "      -- Size of data set: only 369 instances (at that point in time)\n",
    "      -- Applied 4 instance-based learning algorithms \n",
    "      -- Collected classification results averaged over 10 trials\n",
    "      -- Best accuracy result: \n",
    "         -- 1-nearest neighbor: 93.7%\n",
    "         -- trained on 200 instances, tested on the other 169\n",
    "      -- Also of interest:\n",
    "         -- Using only typical instances: 92.2% (storing only 23.1 instances)\n",
    "         -- trained on 200 instances, tested on the other 169\n",
    "\n",
    "4. Relevant Information:\n",
    "\n",
    "   Samples arrive periodically as Dr. Wolberg reports his clinical cases.\n",
    "   The database therefore reflects this chronological grouping of the data.\n",
    "   This grouping information appears immediately below, having been removed\n",
    "   from the data itself:\n",
    "\n",
    "     Group 1: 367 instances (January 1989)\n",
    "     Group 2:  70 instances (October 1989)\n",
    "     Group 3:  31 instances (February 1990)\n",
    "     Group 4:  17 instances (April 1990)\n",
    "     Group 5:  48 instances (August 1990)\n",
    "     Group 6:  49 instances (Updated January 1991)\n",
    "     Group 7:  31 instances (June 1991)\n",
    "     Group 8:  86 instances (November 1991)\n",
    "     -----------------------------------------\n",
    "     Total:   699 points (as of the donated datbase on 15 July 1992)\n",
    "\n",
    "   Note that the results summarized above in Past Usage refer to a dataset\n",
    "   of size 369, while Group 1 has only 367 instances.  This is because it\n",
    "   originally contained 369 instances; 2 were removed.  The following\n",
    "   statements summarizes changes to the original Group 1's set of data:\n",
    "\n",
    "   #####  Group 1 : 367 points: 200B 167M (January 1989)\n",
    "   #####  Revised Jan 10, 1991: Replaced zero bare nuclei in 1080185 & 1187805\n",
    "   #####  Revised Nov 22,1991: Removed 765878,4,5,9,7,10,10,10,3,8,1 no record\n",
    "   #####                  : Removed 484201,2,7,8,8,4,3,10,3,4,1 zero epithelial\n",
    "   #####                  : Changed 0 to 1 in field 6 of sample 1219406\n",
    "   #####                  : Changed 0 to 1 in field 8 of following sample:\n",
    "   #####                  : 1182404,2,3,1,1,1,2,0,1,1,1\n",
    "\n",
    "5. Number of Instances: 699 (as of 15 July 1992)\n",
    "\n",
    "6. Number of Attributes: 10 plus the class attribute\n",
    "\n",
    "7. Attribute Information: (class attribute has been moved to last column)\n",
    "\n",
    "   #  Attribute                     Domain\n",
    "   -- -----------------------------------------\n",
    "   1. Sample code number            id number\n",
    "   2. Clump Thickness               1 - 10\n",
    "   3. Uniformity of Cell Size       1 - 10\n",
    "   4. Uniformity of Cell Shape      1 - 10\n",
    "   5. Marginal Adhesion             1 - 10\n",
    "   6. Single Epithelial Cell Size   1 - 10\n",
    "   7. Bare Nuclei                   1 - 10\n",
    "   8. Bland Chromatin               1 - 10\n",
    "   9. Normal Nucleoli               1 - 10\n",
    "  10. Mitoses                       1 - 10\n",
    "  11. Class:                        (2 for benign, 4 for malignant)\n",
    "\n",
    "8. Missing attribute values: 16\n",
    "\n",
    "   There are 16 instances in Groups 1 to 6 that contain a single missing \n",
    "   (i.e., unavailable) attribute value, now denoted by \"?\".  \n",
    "\n",
    "9. Class distribution:\n",
    " \n",
    "   Benign: 458 (65.5%)\n",
    "   Malignant: 241 (34.5%)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code blocks\n",
    "\n",
    "# sns.countplot(x=\"norm_nuclei\", hue=\"is_malignant\", data=diagnostic_data, palette=\"colorblind\")\n",
    "\n",
    "# create separate kde density plots for benign and malignant\n",
    "#sns.kdeplot(diagnostic_data[diagnostic_data['is_benign'] == 1].mitoses, shade=True, label='benign')\n",
    "# sns.kdeplot(diagnostic_data[diagnostic_data['is_benign'] == 0].mitoses, shade=True, label='malignant')\n",
    "# plot them together\n",
    "# plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
